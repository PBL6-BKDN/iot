import cv2
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
import time
import threading
import onnxruntime as ort
from navigation.speech.voice import VoiceService

# --- Load model ONNX ---
model_path = "model.onnx"
ort_session = ort.InferenceSession(
    model_path,
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)
print("Model ONNX Ä‘Ã£ Ä‘Æ°á»£c load.")

# --- Khá»Ÿi táº¡o VoiceService ---
voice_service = VoiceService()

# --- MÃ u cho tá»«ng lá»›p ---
COLORS = [
    (255, 255, 0),     # lá»›p 0
    (0, 255, 0),       # lá»›p 1
    (255, 0, 0),       # lá»›p 2
    (0, 0, 255),       # lá»›p 3
    (0, 0, 0),         # lá»›p 4 - ná»n
]

# --- HÃ m decode mask sang mÃ u ---
def decode_segmap(mask, num_classes):
    h, w = mask.shape
    color_mask = np.zeros((h, w, 3), dtype=np.uint8)
    for cls in range(num_classes):
        color_mask[mask == cls] = COLORS[cls]
    return color_mask

# --- PhÃ¢n tÃ­ch vá»‹ trÃ­ ngÆ°á»i Ä‘á»©ng dá»±a trÃªn lá»›p ---
def analyze_position(pred):
    guidance = ""
    h, w = pred.shape
    bottom = pred[-h // 4:, :]
    unique_classes = np.unique(bottom)

    if 2 in unique_classes:
        guidance = "âš ï¸ Cáº£nh bÃ¡o: Báº¡n Ä‘ang Ä‘á»©ng trÃªn **Ä‘Æ°á»ng xe cháº¡y**!"
    elif 1 in unique_classes:
        guidance = "ðŸš¸ Báº¡n Ä‘ang Ä‘á»©ng trÃªn **váº¡ch káº» Ä‘Æ°á»ng cho ngÆ°á»i Ä‘i bá»™**."
    elif 3 in unique_classes:
        guidance = "âœ… Báº¡n Ä‘ang Ä‘á»©ng trÃªn **vá»‰a hÃ¨**."
    else:
        guidance = "â“ KhÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c vá»‹ trÃ­ Ä‘á»©ng."

    left = pred[:, :w//3]
    center = pred[:, w//3:2*w//3]
    right = pred[:, 2*w//3:]

    def find_position(region, cls):
        return cls in np.unique(region)

    for cls, name in [(1, "váº¡ch káº» Ä‘Æ°á»ng"), (3, "vá»‰a hÃ¨")]:
        pos = []
        if find_position(left, cls):
            pos.append("bÃªn trÃ¡i")
        if find_position(center, cls):
            pos.append("phÃ­a trÆ°á»›c")
        if find_position(right, cls):
            pos.append("bÃªn pháº£i")

        if pos:
            guidance += f" ðŸ“ ({name}) xuáº¥t hiá»‡n á»Ÿ: {', '.join(pos)}."
        else:
            guidance += f" ðŸ“ ({name}) khÃ´ng xuáº¥t hiá»‡n trong áº£nh."
    return guidance

# --- PhÃ¡t giá»ng nÃ³i ---
def speak_guidance(guidance):
    try:
        if guidance:
            voice_service.speak(guidance)
            print(f"Played guidance: {guidance}")
    except Exception as e:
        print(f"Failed to play audio: {e}")

# --- HÃ m chÃ­nh xá»­ lÃ½ camera ---
def predict_camera(frame, frame_skip=2):
    num_classes = 5
    try:
        last_guidance = ""
        last_speak_time = 0
        frame_count = 0

        print("Äang phÃ¡n Ä‘oÃ¡n lÃ n Ä‘Æ°á»ng...")


        if frame_count % frame_skip != 0:
            frame_count += 1

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        transform = A.Compose([
            A.Resize(384, 512),
            A.Normalize(mean=(0.485, 0.456, 0.406),
                        std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])
        aug = transform(image=frame_rgb)
        input_tensor = aug['image'].unsqueeze(0).cpu().numpy().astype(np.float32)  # (1, 3, 384, 512)

        # Inference ONNX Runtime
        start = time.time()
        ort_inputs = {ort_session.get_inputs()[0].name: input_tensor}
        ort_outs = ort_session.run(None, ort_inputs)
        output = ort_outs[0]  # (1, num_classes, H, W)
        pred = np.argmax(output, axis=1).squeeze()  # (H, W)
        end = time.time()
        print(f"â± Thá»i gian suy luáº­n (ONNX Runtime): {(end - start)*1000:.2f} ms")

        guidance = analyze_position(pred)

        pred_color = decode_segmap(pred, num_classes)
        pred_color_bgr = cv2.cvtColor(pred_color, cv2.COLOR_RGB2BGR)

        img_resized = cv2.resize(frame, (512, 384))
        overlay = cv2.addWeighted(img_resized, 0.5, pred_color_bgr, 0.5, 0)

        if guidance != last_guidance and (time.time() - last_speak_time > 10):
            speak_guidance(guidance)
            last_guidance = guidance
            last_speak_time = time.time()



        frame_count += 1
        return overlay


    except Exception as e:
        print(f"Camera processing failed: {e}")
        if 'cap' in locals() and cap.isOpened():
            cap.release()
        cv2.destroyAllWindows()
        raise